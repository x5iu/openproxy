name: E2E Tests

on:
  push:
    branches: [master]
  pull_request:
    branches: [master]
  workflow_dispatch:

env:
  CARGO_TERM_COLOR: always

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Cache Cargo dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/bin/
            ~/.cargo/registry/index/
            ~/.cargo/registry/cache/
            ~/.cargo/git/db/
          key: ${{ runner.os }}-cargo-deps-${{ hashFiles('**/Cargo.lock') }}

      - name: Run unit tests
        run: cargo test --lib --verbose

  e2e:
    runs-on: ubuntu-latest
    needs: unit-tests
    steps:
      - uses: actions/checkout@v4

      - name: Setup Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Cache Cargo dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/bin/
            ~/.cargo/registry/index/
            ~/.cargo/registry/cache/
            ~/.cargo/git/db/
          key: ${{ runner.os }}-cargo-deps-${{ hashFiles('**/Cargo.lock') }}

      - name: Generate self-signed certificate
        run: |
          openssl req -x509 -newkey rsa:4096 -keyout privkey.pem -out fullchain.pem -days 1 -nodes \
            -subj "/CN=localhost" \
            -addext "subjectAltName=DNS:localhost,IP:127.0.0.1"

      - name: Generate auth key and config
        env:
          OPENAI_HOST: ${{ secrets.OPENAI_HOST }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          GEMINI_HOST: ${{ secrets.GEMINI_HOST }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          ANTHROPIC_HOST: ${{ secrets.ANTHROPIC_HOST }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          # Check required secrets
          if [ -z "$OPENAI_HOST" ]; then
            echo "::error::OPENAI_HOST secret is not set"
            exit 1
          fi
          if [ -z "$OPENAI_API_KEY" ]; then
            echo "::error::OPENAI_API_KEY secret is not set"
            exit 1
          fi
          if [ -z "$GEMINI_HOST" ]; then
            echo "::error::GEMINI_HOST secret is not set"
            exit 1
          fi
          if [ -z "$GEMINI_API_KEY" ]; then
            echo "::error::GEMINI_API_KEY secret is not set"
            exit 1
          fi
          if [ -z "$ANTHROPIC_HOST" ]; then
            echo "::error::ANTHROPIC_HOST secret is not set"
            exit 1
          fi
          if [ -z "$ANTHROPIC_API_KEY" ]; then
            echo "::error::ANTHROPIC_API_KEY secret is not set"
            exit 1
          fi
          AUTH_KEY=$(openssl rand -hex 32)
          echo "AUTH_KEY=${AUTH_KEY}" >> $GITHUB_ENV

          # Process OpenAI config
          OPENAI_ENDPOINT="${OPENAI_HOST#https://}"
          OPENAI_ENDPOINT="${OPENAI_ENDPOINT#http://}"
          OPENAI_ENDPOINT="${OPENAI_ENDPOINT%/}"
          echo "Using OpenAI endpoint: ${OPENAI_ENDPOINT}"

          # Process Gemini config
          GEMINI_ENDPOINT="${GEMINI_HOST#https://}"
          GEMINI_ENDPOINT="${GEMINI_ENDPOINT#http://}"
          GEMINI_ENDPOINT="${GEMINI_ENDPOINT%/}"
          echo "Using Gemini endpoint: ${GEMINI_ENDPOINT}"

          # Process Anthropic config
          ANTHROPIC_ENDPOINT="${ANTHROPIC_HOST#https://}"
          ANTHROPIC_ENDPOINT="${ANTHROPIC_ENDPOINT#http://}"
          ANTHROPIC_ENDPOINT="${ANTHROPIC_ENDPOINT%/}"
          echo "Using Anthropic endpoint: ${ANTHROPIC_ENDPOINT}"

          # Build config file
          cat > config.yml << YAML
          cert_file: fullchain.pem
          private_key_file: privkey.pem
          https_port: 8443
          http_port: 8080

          auth_keys:
            - ${AUTH_KEY}

          providers:
            - type: openai
              host: localhost:8443
              endpoint: ${OPENAI_ENDPOINT}
              api_key: ${OPENAI_API_KEY}
            - type: openai
              host: localhost:8080
              endpoint: ${OPENAI_ENDPOINT}
              api_key: ${OPENAI_API_KEY}
            - type: openai
              host: api.openai.com
              endpoint: ${OPENAI_ENDPOINT}
              api_key: ${OPENAI_API_KEY}
            - type: openai
              host: echo.localhost:8443
              endpoint: localhost
              port: 9000
              tls: false
            - type: openai
              host: echo.localhost:8080
              endpoint: localhost
              port: 9000
              tls: false
            # Providers with path prefix for host path routing tests
            - type: openai
              host: path-test.local/openai
              endpoint: ${OPENAI_ENDPOINT}
              api_key: ${OPENAI_API_KEY}
            - type: openai
              host: nested-path.local/api/v1
              endpoint: ${OPENAI_ENDPOINT}
              api_key: ${OPENAI_API_KEY}
            # H1 fallback test provider (non-TLS upstream)
            - type: openai
              host: h1-fallback.localhost:8443
              endpoint: localhost
              port: 9001
              tls: false
            # Gemini providers
            - type: gemini
              host: gemini.localhost:8443
              endpoint: ${GEMINI_ENDPOINT}
              api_key: ${GEMINI_API_KEY}
            - type: gemini
              host: gemini.localhost:8080
              endpoint: ${GEMINI_ENDPOINT}
              api_key: ${GEMINI_API_KEY}
            # Anthropic providers
            - type: anthropic
              host: anthropic.localhost:8443
              endpoint: ${ANTHROPIC_ENDPOINT}
              api_key: ${ANTHROPIC_API_KEY}
            - type: anthropic
              host: anthropic.localhost:8080
              endpoint: ${ANTHROPIC_ENDPOINT}
              api_key: ${ANTHROPIC_API_KEY}
            # Auth selection test providers (same host, different auth_keys)
            - type: openai
              host: auth-test.local:8443
              endpoint: localhost
              port: 9002
              tls: false
              api_key: sk-api-key-1
              auth_keys:
                - sk-auth-key-1
            - type: openai
              host: auth-test.local:8443
              endpoint: localhost
              port: 9003
              tls: false
              api_key: sk-api-key-2
              auth_keys:
                - sk-auth-key-2
            - type: openai
              host: auth-test.local:8443
              endpoint: localhost
              port: 9004
              tls: false
              api_key: sk-api-key-fallback
              auth_keys:
                - sk-auth-key-fallback
              is_fallback: true
            - type: openai
              host: auth-test.local:8080
              endpoint: localhost
              port: 9002
              tls: false
              api_key: sk-api-key-1
              auth_keys:
                - sk-auth-key-1
            - type: openai
              host: auth-test.local:8080
              endpoint: localhost
              port: 9003
              tls: false
              api_key: sk-api-key-2
              auth_keys:
                - sk-auth-key-2
            - type: openai
              host: auth-test.local:8080
              endpoint: localhost
              port: 9004
              tls: false
              api_key: sk-api-key-fallback
              auth_keys:
                - sk-auth-key-fallback
              is_fallback: true
          YAML

          cat config.yml

      - name: Build openproxy
        run: cargo build --release

      - name: Start openproxy
        run: |
          ./target/release/openproxy start -c config.yml &
          sleep 3
          curl -k https://localhost:8443 || true
          curl http://localhost:8080 || true

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Python dependencies
        run: pip install openai pydantic "httpx[http2]" "websockets==10.4" websocket-client

      - name: Run HTTPS E2E tests
        env:
          OPENAI_BASE_URL: https://localhost:8443/v1
          OPENAI_API_KEY: ${{ env.AUTH_KEY }}
          SSL_CERT_FILE: ${{ github.workspace }}/fullchain.pem
          REQUESTS_CA_BUNDLE: ${{ github.workspace }}/fullchain.pem
        run: |
          cd e2e
          python test_https.py

      - name: Run HTTP/2 large body E2E test
        env:
          OPENAI_BASE_URL: https://localhost:8443/v1
          OPENAI_API_KEY: ${{ env.AUTH_KEY }}
          SSL_CERT_FILE: ${{ github.workspace }}/fullchain.pem
          REQUESTS_CA_BUNDLE: ${{ github.workspace }}/fullchain.pem
        run: |
          cd e2e
          python test_h2_large_body.py

      - name: Run HTTP/2 upstream E2E tests
        env:
          OPENAI_BASE_URL: https://localhost:8443/v1
          OPENAI_API_KEY: ${{ env.AUTH_KEY }}
          SSL_CERT_FILE: ${{ github.workspace }}/fullchain.pem
          REQUESTS_CA_BUNDLE: ${{ github.workspace }}/fullchain.pem
        run: |
          cd e2e
          python test_h2_upstream.py

      - name: Run HTTP E2E tests
        env:
          OPENAI_BASE_URL: http://localhost:8080/v1
          OPENAI_API_KEY: ${{ env.AUTH_KEY }}
        run: |
          cd e2e
          python test_http.py

      - name: Run Host Path Prefix E2E tests (HTTPS)
        env:
          PROXY_HOST: localhost
          PROXY_HTTPS_PORT: 8443
          PROXY_HTTP_PORT: 8080
          OPENAI_API_KEY: ${{ env.AUTH_KEY }}
          SSL_CERT_FILE: ${{ github.workspace }}/fullchain.pem
          REQUESTS_CA_BUNDLE: ${{ github.workspace }}/fullchain.pem
        run: |
          cd e2e
          python test_host_path.py

      - name: Start WebSocket echo server
        run: |
          cd e2e
          python websocket_echo_server.py --host 127.0.0.1 --port 9000 &
          sleep 2
          echo "WebSocket echo server started on ws://127.0.0.1:9000"

      - name: Verify echo server is running
        run: |
          cat > /tmp/test_echo.py << 'PYTEST'
          import asyncio
          import websockets
          async def test():
              async with websockets.connect('ws://127.0.0.1:9000') as ws:
                  await ws.send('test')
                  resp = await ws.recv()
                  print(f'Echo server test: sent "test", received "{resp}"')
                  assert resp == 'test', 'Echo server not working'
                  print('Echo server is working correctly')
          asyncio.run(test())
          PYTEST
          python /tmp/test_echo.py

      - name: Run WebSocket E2E tests (HTTP)
        env:
          WS_URL: ws://localhost:8080
          WS_HOST: echo.localhost:8080
          OPENAI_API_KEY: ${{ env.AUTH_KEY }}
        run: |
          cd e2e
          python test_websocket.py

      - name: Run WebSocket E2E tests (HTTPS)
        env:
          WSS_URL: wss://localhost:8443
          WSS_HOST: echo.localhost:8443
          SSL_CERT_FILE: ${{ github.workspace }}/fullchain.pem
          OPENAI_API_KEY: ${{ env.AUTH_KEY }}
        run: |
          cd e2e
          python test_websocket.py

      - name: Run OpenAI Realtime API test
        env:
          OPENAI_API_KEY: ${{ env.AUTH_KEY }}
          SSL_CERT_FILE: ${{ github.workspace }}/fullchain.pem
        run: |
          cd e2e
          python test_openai_realtime.py --proxy-host localhost --proxy-port 8443 --ssl-cert $SSL_CERT_FILE --timeout 60

      - name: Start HTTP echo server for fallback test
        run: |
          # Start a simple HTTP echo server on port 9001
          python3 -c '
          from http.server import HTTPServer, BaseHTTPRequestHandler
          import json

          class EchoHandler(BaseHTTPRequestHandler):
              protocol_version = "HTTP/1.1"

              def do_GET(self):
                  response = {"path": self.path, "method": "GET", "fallback": "success"}
                  body = json.dumps(response).encode()
                  self.send_response(200)
                  self.send_header("Content-Type", "application/json")
                  self.send_header("Content-Length", str(len(body)))
                  self.end_headers()
                  self.wfile.write(body)

              def do_POST(self):
                  content_length = int(self.headers.get("Content-Length", 0))
                  req_body = self.rfile.read(content_length)
                  response = {"path": self.path, "method": "POST", "fallback": "success"}
                  body = json.dumps(response).encode()
                  self.send_response(200)
                  self.send_header("Content-Type", "application/json")
                  self.send_header("Content-Length", str(len(body)))
                  self.end_headers()
                  self.wfile.write(body)

              def log_message(self, format, *args):
                  pass  # Suppress logging

          HTTPServer(("127.0.0.1", 9001), EchoHandler).serve_forever()
          ' &
          sleep 2
          echo "HTTP echo server started on http://127.0.0.1:9001"

      - name: Run H2 to H1 fallback test
        env:
          PROXY_AUTH_KEY: ${{ env.AUTH_KEY }}
          SSL_CERT_FILE: ${{ github.workspace }}/fullchain.pem
        run: |
          cd e2e
          python test_h2_h1_fallback.py

      - name: Run Gemini E2E tests (HTTPS with curl)
        env:
          PROXY_AUTH_KEY: ${{ env.AUTH_KEY }}
          SSL_CERT_FILE: ${{ github.workspace }}/fullchain.pem
        run: |
          echo "Testing Gemini API via HTTPS proxy with HTTP/2..."

          # Test 1: List models with x-goog-api-key header authentication (HTTP/2)
          echo "Test 1: Gemini list models via H2 proxy"
          curl -s -D /tmp/gemini_headers.txt -o /tmp/gemini_response.json \
            --cacert $SSL_CERT_FILE \
            --http2 \
            -H "Host: gemini.localhost:8443" \
            -H "x-goog-api-key: ${PROXY_AUTH_KEY}" \
            "https://localhost:8443/v1beta/models?key=placeholder&pageSize=1"

          HTTP_CODE=$(head -1 /tmp/gemini_headers.txt | grep -oE '[0-9]{3}')
          echo "HTTP Status: $HTTP_CODE"
          echo "Response headers:"
          cat /tmp/gemini_headers.txt
          echo "Response body:"
          cat /tmp/gemini_response.json | head -c 500
          echo ""

          if [ "$HTTP_CODE" != "200" ]; then
            echo "::error::Gemini H2 test failed with status $HTTP_CODE"
            cat /tmp/gemini_response.json
            exit 1
          fi

          # Verify response contains models
          if ! grep -q "models" /tmp/gemini_response.json; then
            echo "::error::Gemini response does not contain models"
            cat /tmp/gemini_response.json
            exit 1
          fi

          # Verify upstream protocol is H2
          if ! grep -qi "x-upstream-protocol: h2" /tmp/gemini_headers.txt; then
            echo "::error::Expected x-upstream-protocol: h2 header but not found"
            cat /tmp/gemini_headers.txt
            exit 1
          fi

          echo "Gemini HTTPS/H2 test passed! (verified upstream protocol is H2)"

      - name: Run Gemini E2E tests (HTTP with curl)
        env:
          PROXY_AUTH_KEY: ${{ env.AUTH_KEY }}
        run: |
          echo "Testing Gemini API via HTTP proxy..."

          # Test: List models with x-goog-api-key header authentication (HTTP/1.1)
          echo "Test: Gemini list models via HTTP/1.1 proxy"
          HTTP_CODE=$(curl -s -o /tmp/gemini_response_http.json -w "%{http_code}" \
            -H "Host: gemini.localhost:8080" \
            -H "x-goog-api-key: ${PROXY_AUTH_KEY}" \
            "http://localhost:8080/v1beta/models?key=placeholder&pageSize=1")

          echo "HTTP Status: $HTTP_CODE"
          cat /tmp/gemini_response_http.json | head -c 500
          echo ""

          if [ "$HTTP_CODE" != "200" ]; then
            echo "::error::Gemini HTTP test failed with status $HTTP_CODE"
            cat /tmp/gemini_response_http.json
            exit 1
          fi

          # Verify response contains models
          if ! grep -q "models" /tmp/gemini_response_http.json; then
            echo "::error::Gemini response does not contain models"
            cat /tmp/gemini_response_http.json
            exit 1
          fi

          echo "Gemini HTTP/1.1 test passed!"

      - name: Install PyYAML and anthropic for Anthropic tests
        run: pip install pyyaml anthropic

      - name: Run Anthropic OAuth E2E tests
        run: |
          echo "Testing Anthropic OAuth authentication..."
          cd e2e
          python test_anthropic_oauth.py
          echo "Anthropic OAuth E2E tests passed!"

      - name: Run Anthropic API E2E tests
        env:
          PROXY_HTTPS_URL: https://localhost:8443
          PROXY_HTTP_URL: http://localhost:8080
          ANTHROPIC_HOST: anthropic.localhost
          OPENAI_API_KEY: ${{ env.AUTH_KEY }}
          SSL_CERT_FILE: ${{ github.workspace }}/fullchain.pem
          REQUESTS_CA_BUNDLE: ${{ github.workspace }}/fullchain.pem
        run: |
          echo "Testing Anthropic API with multiple auth methods..."
          cd e2e
          python test_anthropic_api.py
          echo "Anthropic API E2E tests passed!"

      - name: Run Hot Upgrade E2E tests
        env:
          OPENPROXY_BINARY: ${{ github.workspace }}/target/release/openproxy
          OPENAI_HOST: ${{ secrets.OPENAI_HOST }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          SSL_CERT_FILE: ${{ github.workspace }}/fullchain.pem
        run: |
          echo "Testing hot upgrade (SIGUSR2)..."
          cd e2e
          python test_hot_upgrade.py
          echo "Hot upgrade E2E tests passed!"

      - name: Start echo servers for auth selection test
        run: |
          # Start echo servers that return their server_id
          for port in 9002 9003 9004; do
            server_id="provider$((port - 9001))"
            if [ "$port" == "9004" ]; then server_id="fallback"; fi
            python3 -c "
          from http.server import HTTPServer, BaseHTTPRequestHandler
          import json

          class Handler(BaseHTTPRequestHandler):
              protocol_version = 'HTTP/1.1'
              def do_GET(self):
                  body = json.dumps({'server_id': '$server_id', 'path': self.path}).encode()
                  self.send_response(200)
                  self.send_header('Content-Type', 'application/json')
                  self.send_header('Content-Length', str(len(body)))
                  self.end_headers()
                  self.wfile.write(body)
              def do_POST(self):
                  self.do_GET()
              def log_message(self, *args):
                  pass

          HTTPServer(('127.0.0.1', $port), Handler).serve_forever()
          " &
          done
          sleep 2
          echo "Echo servers started on ports 9002, 9003, 9004"

      - name: Run Auth Selection E2E tests
        env:
          PROXY_HTTPS_PORT: 8443
          PROXY_HTTP_PORT: 8080
          SSL_CERT_FILE: ${{ github.workspace }}/fullchain.pem
        run: |
          echo "Testing auth-during-provider-selection..."
          cd e2e
          python test_auth_selection.py
          echo "Auth selection E2E tests passed!"

      - name: Run Health Check Auth E2E tests
        env:
          OPENPROXY_BINARY: ${{ github.workspace }}/target/release/openproxy
        run: |
          echo "Testing health check auth error handling..."
          cd e2e
          python test_health_check_auth.py
          echo "Health check auth E2E tests passed!"

      - name: Run CONNECT tunnel disabled test
        env:
          PROXY_HOST: localhost
          PROXY_PORT: 8080
          OPENAI_API_KEY: ${{ env.AUTH_KEY }}
          TEST_CONNECT_DISABLED: "true"
        run: |
          echo "Testing CONNECT tunnel when disabled..."
          cd e2e
          python test_connect_tunnel.py
          echo "CONNECT tunnel disabled test passed!"

      - name: Create CONNECT-enabled config and start second proxy
        env:
          OPENAI_HOST: ${{ secrets.OPENAI_HOST }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          # Process OpenAI config
          OPENAI_ENDPOINT="${OPENAI_HOST#https://}"
          OPENAI_ENDPOINT="${OPENAI_ENDPOINT#http://}"
          OPENAI_ENDPOINT="${OPENAI_ENDPOINT%/}"

          # Create config with connect_tunnel_enabled
          cat > config_connect.yml << YAML
          cert_file: fullchain.pem
          private_key_file: privkey.pem
          https_port: 18443
          http_port: 18080
          connect_tunnel_enabled: true

          auth_keys:
            - ${AUTH_KEY}

          providers:
            # For CONNECT tunnel, tls must be false so proxy does transparent TCP forwarding
            # and client handles TLS directly with upstream
            - type: openai
              host: ${OPENAI_ENDPOINT}
              endpoint: ${OPENAI_ENDPOINT}
              api_key: ${OPENAI_API_KEY}
              tls: false
            # Echo server provider for data transfer test
            - type: openai
              host: local-service
              endpoint: localhost
              port: 19999
              tls: false
          YAML

          cat config_connect.yml

          # Start second openproxy instance with CONNECT enabled
          ./target/release/openproxy start -c config_connect.yml &
          sleep 3
          curl http://localhost:18080 || true
          echo "CONNECT-enabled proxy started on port 18080"

      - name: Run CONNECT tunnel enabled tests
        env:
          PROXY_HOST: localhost
          PROXY_PORT_CONNECT: 18080
          OPENAI_API_KEY: ${{ env.AUTH_KEY }}
          TARGET_HOST: ${{ secrets.OPENAI_HOST }}
          TARGET_PORT: "443"
          TEST_CONNECT_ENABLED: "true"
          TEST_CONNECT_AUTH_FAILURE: "true"
          TEST_CONNECT_NO_PROVIDER: "true"
        run: |
          echo "Testing CONNECT tunnel when enabled..."
          cd e2e
          python test_connect_tunnel.py
          echo "CONNECT tunnel enabled tests passed!"

      - name: Run CONNECT tunnel data transfer and preread tests
        env:
          PROXY_HOST: localhost
          PROXY_PORT_CONNECT: 18080
          OPENAI_API_KEY: ${{ env.AUTH_KEY }}
          ECHO_TARGET_HOST: local-service
          ECHO_TARGET_PORT: "19999"
          TEST_CONNECT_DATA_TRANSFER: "true"
          TEST_CONNECT_PREREAD: "true"
        run: |
          echo "Testing CONNECT tunnel data transfer and preread..."
          cd e2e
          python test_connect_tunnel.py
          echo "CONNECT tunnel data transfer and preread tests passed!"
